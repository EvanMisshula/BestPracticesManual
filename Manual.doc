<p>Comments and suggestions are encouraged. Please send correspondence to garret@berkeley.edu, or find the latest version of the manual on <a href="https://github.com/garretchristensen/BestPracticesManual">github</a>.</p>
<h1 id="introduction">Introduction</h1>
<p>Scientific claims should be subject to scrutiny by other researchers and the public at large. An essential requirement for such scrutiny is that researchers make their claims transparent in a way that other researchers are able to use easily available resources to form a complete understanding of the methods that were used by the original. In the social sciences, especially given the personal computing and Internet revolutions and the wide availability of data and processing power, it is essential that data, code, and analyses be transparent.</p>
<p>This manual is intended to be a source mainly for empirical social science researchers who desire to make their own research transparent to, and reproducible by, others. The entire process of research, from hypothesis generation to publication, is covered. Although norms differ across disciplines, we attempt to bring a broad view of the empirical social sciences to these recommendations, and hope that students and researchers in any social science field may tailor these recommendations to best fit their field.</p>
<p>In section [ethical-research] we first discuss the motivation for this document: the desire to do ethical research. A major component of ethical social science research is treating research subjects appropriately. This is mandated by federal law and overseen by Institutional Review Boards (IRBs), and should be taken seriously by researchers. But just as treating subjects fairly is ethical, we believe that transparent, reproducible research is also a major part of ethical research.</p>
<p>In section [study-design] we discuss study design, including how to power studies appropriately.</p>
<p>In section [registration] we discuss one of the major problems in non-transparent research, specifically publication bias. We also discuss how this problem can be resolved through the practice of registration. Publication bias stems from the fact that published results are overwhelmingly statistically significant. But without knowing how many tests were run (the number of unpublished results), it is impossible to know whether these significant results are meaningful, or whether they are the 5% of tests that we would expect to appear significant due to random sampling, even with no true effect. By publicly registering all studies, we can have a better idea of just how many tests have been run.</p>
<p>In section [rdof] we discuss researcher degrees of freedom and pre-analysis plans; In addition to registering trials, researchers should also specify their outcomes of interest and their exact methods of analysis to bind their hands during the analysis phase by writing a Pre-Analysis Plan (PAP). This is a relatively new idea in the social sciences, so there is not yet a consensus on when a PAP should be required, what the ideal level of detail is, and how much it should constrain the researchers hand in the actual analysis, but by pre-specifying analyses, researchers can distinguish between confirmatory and exploratory analysis. We do not necessarily place higher intrinsic value on one or the other, but making the distinction clear is key for appropriate interpretation.</p>
<p>In section [replication-and-reproducibility] we discuss workflow and materials sharing, with an eye on making research replicable by others. Researchers should make their code and data publicly available so that others may repeat and verify their analysis. Making data available incentivizes researchers to make their work accurate in the first place, and makes replication easier for others, improving the scientific process, but also raises the concern of differential privacy, since steps should be taken to prevent identification of individuals in the data. We also discuss the issue of reporting standards: a standardized list of things that authors should report to help make their work reproducible.</p>
<p>Section [conclusion] concludes and presents a vision for moving forward.</p>
<h1 id="ethical-research">Ethical Research</h1>
<p>We believe that making one’s research transparent and reproducible is a key component of ethical research.</p>
<h2 id="fraud">Fraud</h2>
<p>While most of us are likely to presume that we ourselves would not conduct outright fraud, fraud does indeed occur. From making up fake data to creating bogus e-mail addresses so one could do one’s own peer review, the Retraction Watch blog (<a href="http://www.retractionwatch.com" class="uri">http://www.retractionwatch.com</a>) documents a distressingly large amount of deliberate fraud in research. Although the blog tends to specialize in the life sciences, there is no good reason to believe that social science researchers are inherently more benevolent. Part of the US Department of Health and Human Services , the Office of Research Integrity (ORI), works to promote research integrity and document misconduct, especially when it involves federally funded research. The misconduct case summaries of the ORI (<a href="http://ori.hhs.gov/case_summary" class="uri">http://ori.hhs.gov/case_summary</a>), and the stories of Diederik Stapel <span class="citation"></span> Hwang Woo-Suk <span class="citation"></span> and Marc Hauser <span class="citation"></span> should be sobering warnings to us all.</p>
<h2 id="unintentional-bias">Unintentional Bias</h2>
<p>Perhaps in addition to the obvious need to avoid deliberate fraud and protect our human subjects is the need to avoid subconsciously biasing our own results.</p>
<p><span class="citation"></span> summarize some of the evidence on this subject, concluding that there are many circumstances common to academia and the publishing paradigm that cause researchers to frequently use motivated reasoning:</p>
<blockquote>
<p>Because we have directional goals for success, we are likely to bring to bear motivated reasoning to justify research decisions in the name of accuracy, when they are actually in service of career advancement (Fanelli, 2010a). Motivated reasoning is particularly influential when the situation is complex, the available information is ambiguous, and legitimate reasons can be generated for multiple courses of action (Bersoff, 1999; Boiney, Kennedy, &amp; Nye, 1997; Kunda, 1990).</p>
<p>Motivated reasoning can occur without intention. We are more likely to be convinced that our hypothesis is true, accepting uncritically when it is confirmed and crutinizing heavily when it is not (Bastardi, Uhlmann, &amp; Ross, 2011; Ditto &amp; Lopez, 1992; Lord, Ross, &amp; Lepper, 1979; Pyszczynski &amp; Greenberg, 1987; Trope &amp; Bassok, 1982). With flexible analysis options, we are more likely to find the one that produces a more publishable pattern of results to be more reasonable and defensible than others (Simmons et al., 2011; Wagenmakers, Wetzels, Borsboom, &amp; van der Maas, 2011). Once we obtain an unexpected result, we are likely to reconstruct our histories and perceive the outcome as something that we could have, even did, anticipate all along—converting a discovery into a confirmatory result (Fischoff, 1977; Fischoff &amp; Beyth, 1975). And even if we resist those reasoning biases in the moment, after a few months, we might simply forget the details, whether we had hypothesized the moderator, had good justification for one set of exclusion criteria compared with another, and had really thought that the one dependent variable that showed a significant effect was the key outcome. Instead, we might remember the gist of what the study was and what we found (Reyna &amp; Brainerd, 1995). Forgetting the details provides an opportunity for reimagining the study purpose and results to recall and understand them in their best (i.e., most publishable) light. The reader may, as we do, recall personal examples of such motivated decisions—they are entirely ordinary products of human cognition.</p>
</blockquote>
<h2 id="institutional-review-boards">Institutional Review Boards</h2>
<p>In addition to fraud, a major ethical concern relates to our human subjects.</p>
<h3 id="history">History</h3>
<p>World history is rife with examples of atrocities conducted in the name of research. Some of these have resulted in major changes in regulations related to research.</p>
<h4 id="nuremberg">Nuremberg</h4>
<p>Nazi German doctors conducted horrible experiments on subjects during World War II. The “Doctor’s Trial” (USA v. Karl Brandt, et al.) tried 23 defendants, and the verdict included the following ten principles, which although never entered as formal regulations in either Germany or the USA, became widely accepted.</p>
<ol>
<li><p>The voluntary consent of the human subject is absolutely essential.</p>
<p>This means that the person involved should have legal capacity to give consent; should be so situated as to be able to exercise free power of choice, without the intervention of any element of force, fraud, deceit, duress, over-reaching, or other ulterior form of constraint or coercion; and should have sufficient knowledge and comprehension of the elements of the subject matter involved as to enable him to make an understanding and enlightened decision. This latter element requires that before the acceptance of an affirmative decision by the experimental subject there should be made known to him the nature, duration, and purpose of the experiment; the method and means by which it is to be conducted; all inconveniences and hazards reasonably to be expected; and the effects upon his health or person which may possibly come from his participation in the experiment.</p>
<p>The duty and responsibility for ascertaining the quality of the consent rests upon each individual who initiates, directs or engages in the experiment. It is a personal duty and responsibility which may not be delegated to another with impunity.</p></li>
<li><p>The experiment should be such as to yield fruitful results for the good of society, unprocurable by other methods or means of study, and not random and unnecessary in nature.</p></li>
<li><p>The experiment should be so designed and based on the results of animal experimentation and a knowledge of the natural history of the disease or other problem under study that the anticipated results will justify the performance of the experiment.</p></li>
<li><p>The experiment should be so conducted as to avoid all unnecessary physical and mental suffering and injury.</p></li>
<li><p>No experiment should be conducted where there is an a priori reason to believe that death or disabling injury will occur; except, perhaps, in those experiments where the experimental physicians also serve as subjects.</p></li>
<li><p>The degree of risk to be taken should never exceed that determined by the humanitarian importance of the problem to be solved by the experiment.</p></li>
<li><p>Proper preparations should be made and adequate facilities provided to protect the experimental subject against even remote possibilities of injury, disability, or death.</p></li>
<li><p>The experiment should be conducted only by scientifically qualified persons. The highest degree of skill and care should be required through all stages of the experiment of those who conduct or engage in the experiment.</p></li>
<li><p>During the course of the experiment the human subject should be at liberty to bring the experiment to an end if he has reached the physical or mental state where continuation of the experiment seems to him to be impossible.</p></li>
<li><p>During the course of the experiment the scientist in charge must be prepared to terminate the experiment at any stage, if he has probably cause to believe, in the exercise of the good faith, superior skill and careful judgment required of him that a continuation of the experiment is likely to result in injury, disability, or death to the experimental subject.</p></li>
</ol>
<h4 id="tuskegee-and-us-codification">Tuskegee and US codification</h4>
<p>In 1972 whistleblower Peter Buxton revealed to the Associated Press that the US Public Health Service was conducting a 40-year experiment on poor Alabama sharecroppers in which it did not treat those who had syphilis for the disease despite the discovery and verification of penicillin as an effective treatment, and actually prevented sufferers from obtaining treatment elsewhere. As a result, the National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research was formed by law in 1974, and released the Belmont Report in 1979. The Belmont Report, available at <a href="http://www.hhs.gov/ohrp/humansubjects/guidance/belmont.html" class="uri">http://www.hhs.gov/ohrp/humansubjects/guidance/belmont.html</a>, contains three basic ethical principles, and three applications:</p>
<ul>
<li><p>Ethical Principles</p>
<ul>
<li><p>Respect for Persons: “Respect for persons incorporates at least two ethical convictions: first, that individuals should be treated as autonomous agents, and second, that persons with diminished autonomy are entitled to protection.”</p></li>
<li><p>Beneficence: “Two general rules have been formulated as complementary expressions of beneficent actions in this sense: <strong>(1)</strong> do not harm and <strong>(2)</strong> maximize possible benefits and minimize possible harms.”</p></li>
<li><p>Justice: “An injustice occurs when some benefit to which a person is entitled is denied without good reason or when some burden is imposed unduly. Another way of conceiving the principle of justice is that equals ought to be treated equally.”</p></li>
</ul></li>
<li><p>Applications</p>
<ul>
<li><p>Informed Consent: “Respect for persons requires that subjects, to the degree that they are capable, be given the opportunity to choose what shall or shall not happen to them. This opportunity is provided when adequate standards for informed consent are satisfied.”</p></li>
<li><p>Assessment of Risks and Benefits: “It is commonly said that benefits and risks must be”balanced“ and shown to be “in a favorable ratio.” The metaphorical character of these terms draws attention to the difficulty of making precise judgments. Only on rare occasions will quantitative techniques be available for the scrutiny of research protocols. However, the idea of systematic, nonarbitrary analysis of risks and benefits should be emulated insofar as possible.”</p></li>
<li><p>Selection of Subjects: “Individual justice in the selection of subjects would require that researchers exhibit fairness: thus, they should not offer potentially beneficial research only to some patients who are in their favor or select only”undesirable“ persons for risky research. Social justice requires that distinction be drawn between classes of subjects that ought, and ought not, to participate in any particular kind of research, based on the ability of members of that class to bear burdens and on the appropriateness of placing further burdens on already burdened persons.”</p></li>
</ul></li>
</ul>
<p>In 1981 the Department of Health and Human Services and the Food and Drug Administration adopted regulations in line with the Belmont report, and 15 federal agencies adopted these regulations (45 CFR part 46) as the “Common Rule” in 1991. See <a href="http://www.hhs.gov/ohrp/index.html" class="uri">http://www.hhs.gov/ohrp/index.html</a> for more information.</p>
<p>In practice, this means that researchers who receive funding from the US government, or who work at institutions that receive federal funding (i.e. essentially all researchers) should have their research approved by an Institutional Review Board (IRB). IRB are a decentralized approval body set up by each research organization itself, consisting of at least five members, a mix of men and women, scientists and non-scientists, and at least one member not affiliated with the institution. Since IRBs and the approval process are decentralized, the exact process varies from institution to institution, but one example can be seen at <a href="http://cphs.berkeley.edu" class="uri">http://cphs.berkeley.edu</a>.</p>
<p>When conducting research internationally, researchers should give their human subjects the same protections as those inside the US. Laws in developing countries may not be as well-defined or enforced, but researchers should still register with their US institution’s IRB, and obtain approval from the host country government. A list of laws and regulations that cover research in 107 foreign countries is available from the Office for Human Research Protections at <a href="http://www.hhs.gov/ohrp/international/intlcompilation/2014intlcomp.pdf.pdf" class="uri">http://www.hhs.gov/ohrp/international/intlcompilation/2014intlcomp.pdf.pdf</a>.</p>
<p>Another key resource for researchers and research conducted outside the US is the Declaration of Helsinki by the World Medical Association (WMA), available at <a href="http://www.wma.net/en/30publications/10policies/b3/index.html" class="uri">http://www.wma.net/en/30publications/10policies/b3/index.html</a>. Originally adopted by the WMA in 1964, the document has significantly influenced the laws and regulations adopted to govern research worldwide.</p>
<p>Lest one think that ethical concerns are limited to monsters of bygone eras, we refer readers to a dilemma caused by an election experiment by researchers from Stanford and Dartmouth in 2014: <a href="http://www.washingtonpost.com/blogs/monkey-cage/wp/2014/11/03/ethics-and-research-in-comparative-politics/" class="uri">http://www.washingtonpost.com/blogs/monkey-cage/wp/2014/11/03/ethics-and-research-in-comparative-politics/</a>.</p>
<h3 id="training">Training</h3>
<p>A large number of universities participate in the Collaborative Institutional Training Initiative at the University of Miami (CITI, <a href="https://www.citiprogram.org/" class="uri">https://www.citiprogram.org/</a>). Completing their course on Human Subjects Research is often a requirement of being included on a research proposal.</p>
<h1 id="study-design">Study Design</h1>
<h2 id="Conducting Power Analysis">Power Analysis</h2>
<p>Given that researchers are working within a null hypothesis testing framework, the ‘power’ of a study, the probability of rejecting the null hypothesis when it is false, is extremely important. Though 80% power is held as a lower bound for acceptable power in many disciplines such as medical research, the actual power of studies can be much lower. For example, research by <span class="citation"></span> found that the median power in Neuroscience was 21%. This means that if a study investigating a true effect were run 100 time, 79 studies would fail to reach significance, meaning that the chance of a false negative is extremely high.</p>
<p>Though false negatives probably the most often discussed issue with low powered studies, they are not the only issue. A second issue is that low powered studies actually decrease the likelihood of a true positive <span class="citation"></span>. This means that when a low powered studies does find and effect, the lower the likelihood that this effect is true in the population; it is more likely that it is a false positive. A third issue with low powered studies relates to effect sizes. Small, low powered studies that reach statistical significance will over-estimate the true effect size in the population <span class="citation"></span>. These inflated effect size estimates will make it difficult to properly power future studies, and the inaccuracy of the estimate may also be problematic for making decisions based on scientific results.</p>
<p>An obvious way to help mitigate the problems of underpowered studies is to run studies with higher power. However, this process is not always that straightforward. As previously mentioned, effect sizes from published studies are often inflated, and so may not provide the most accurate estimate. Meta-analyses can provide better information, but they also often suffer from publication bias and thus inflated effect sizes. Additionally, effect sizes may be highly heterogeneous between studies, even studies with the same materials and methodologies <span class="citation"></span>. Thus, using a single point estimate of an effect size from published literature may lead to inaccurate power calculations.</p>
<p>To combat these problems alternatives to the ‘standard’ power analysis have been suggested. For example, <span class="citation"></span> have suggested a technique called ‘safeguard power’ which takes into account the uncertainty surrounding effect size estimates when conducting power analyses. Specifically, they suggest basing power calculations off of the effect size corresponding to the lower bound of a 60% confidence interval around the point estimate from published literature. Another approach by <span class="citation"></span> takes into account the between study variation in effect sizes when conducting power analyses to give a more conservative estimate of the number of participants needed to reach a given level of statistical power. Yet another approach, which is also feasible when an effect size estimate from the previous literature is not available, is to determine the smallest effect size you wish to be able to detect, and power your study to find this effect size.</p>
<h1 id="registration">Registration</h1>
<p>One of the problems brought into focus recently is publication bias. Publication bias is the selective publication of only significant results. Thankfully, there are tools available for researchers to combat these problems.</p>
<h2 id="publication-bias">Publication Bias</h2>
<p>One of the primary drivers of the recent move towards transparency is increased awareness of publication bias. Numerous papers use collections of published papers to show that the proportion of significant results are extremely unlikely to come from any true population distribution <span class="citation"></span>. By examining the publication rates of null results and significant results from a large set of NSF-funded studies, <span class="citation"></span> show that the selective publication of only significant results may stem from the fact that social science researchers largely fail to write up and submit results from studies resulting in null findings, citing lack of interest or fear of rejection. This idea of rejecting, or not even submitting for review, papers with null-results, is commonly referred to as the “file drawer problem.” In fact, the percentage of null findings published in journals appeas to have been decreasing over time, across all disciplines <span class="citation"></span>. Clearly, there is no reason why this would be an accurate reflection of the state of the universe. If journals only publish statistically significant results, we have no idea how many of those significant results are evidence of real effects, and which are the 5% of random draws that we should expect to show a significant result with a true zero effect. One way to combat this problem is to require registration of all studies undertaken. Ideally we then search the registry for studies of X on Y. If numerous studies show an effect, we have confidence the effect is real. If 5% of studies show a significant effect, we give these outliers studies less credence.</p>
<h2 id="trial-registration">Trial Registration</h2>
<p>A basic definition of registration is to publicly declare <em>all</em> research that one plans on conducting. Ideally this is done in a public registry designed to accept registrations in the given research discipline, and ideally the registration takes place before data collection begins.</p>
<p>Almost all registration efforts have thus far been limited to randomized control trials, as opposed to observational data. However, we believe that registering all types of analysis should be accepted and encouraged. Registration of randomized trials has achieved wide adoption in medicine, but is still relatively new to the social sciences. After congress passed a law in 1997 requiring the creation of a registry for FDA-regulated trials, and the NIH created clinicaltrials.gov in 2000, The International Committee of Medical Journal Editors (ICMJE), a collection of editors of top medical journals, instituted a policy of publishing only registered trials in 2005 <span class="citation"></span>, and the policy has spread to other journals and been generally accepted by researchers <span class="citation"></span>.</p>
<p>A profound example of the benefit of trial registries is detailed in <span class="citation"></span>, which details the publication rates of studies related to FDA-approved antidepressants. (See also <span class="citation"></span>.) The outcome is perhaps what the most hardened cynic would expect: essentially all the trials with positive outcomes were published, a 50/50 mix of questionable-outcome studies were published, and a majority of the negative-outcome studies were unpublished a minimum of four years after the study was completed. The figure below shows the drastically different rates of publication, and a large amount of publication bias.</p>
<p><img src="TurnerFigure1.PNG" alt="image" /></p>
<p>Panel A of Figure 1 from <span class="citation"></span></p>
<p>Of course for this sort of exercise to be possible, unless a reader merely assumes that a registered trial without an associated published paper produced a null result, it requires that the registration site itself obtain outcomes of trials. <a href="http://www.clinicaltrials.gov">ClinicalTrials.gov</a> is the only publicly available trial registry that requires such reporting of results, and only for certain FDA trials.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> <span class="citation"></span> raises concerns about discrepancies between reporting of outcomes in published papers and in the <a href="http://www.clinicaltrials.gov">ClinicalTrials.gov</a> database; as many as 20% of studies had discrepancies in primary outcomes and as many as 33% had discrepancies in reporting of adverse events.</p>
<p>Even with dramatic growth in medical trial registration, problems remain. Not all journals have adopted the ICMJE policy, and complete enforcement is elusive. <span class="citation"></span> looked at trials related to three medical conditions and found that only 46% of studies were registered before the end of the trial with primary outcomes clearly specified. Even among those adequately registered, 31% showed some discrepancies between registered and published outcomes, with bias in favor of statistically significant definitions.</p>
<h2 id="social-science-registries">Social Science Registries</h2>
<p>Registries in the social sciences are newer but are growing ever more popular. The Abdul Latif Jameel Poverty Action Lab began hosting a hypothesis registry (<a href="http://www.povertyactionlab.org/hypothesis-registry" class="uri">http://www.povertyactionlab.org/hypothesis-registry</a>) in 2009, which was superseded by the American Economic Association’s launch of its own registry for randomized trials (<a href="http://www.socialscienceregistry.org">www.socialscienceregistry.org</a>) in May 2013, which had accumulated 260 studies in 59 countries by October 2014. The International Initiative for Impact Evaluation (3ie) launched its own registry for evaluations of development programs, the Registry for International Development Impact Evaluations (RIDIE, <a href="http://ridie.3ieimpact.org" class="uri">http://ridie.3ieimpact.org</a>) in September 2013, which had approximately 30 evaluations registered in its first year.</p>
<p>In political science, EGAP: Experiments in Governance and Politics has created a registry as “an unsupervised stopgap function to store designs until the creation of a general registry for social science research. The EGAP registry focuses on designs for experiments and observational studies in governance and politics.” (<a href="http://e-gap.org/design-registration" class="uri">http://e-gap.org/design-registration</a>) EGAP’s registry had 93 designs registered as of October 2014.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>Another location for registrations is the <a href="http://osf.io">Open Science Framework</a> (OSF), created by the <a href="http://centerforopenscience.org/">Center for Open Science</a>. The OSF serves as a broad research management tool that encourages and facilitates transparency (see <span class="citation"></span>.) Registrations are simply unalterable snapshots of research frozen in time, with a persistent URL and timestamp. Researchers can upload their data, code, hypotheses, etc. to the OSF, register it, and then share the resulting URL as proof of registration. OSF registrations can be relatively free-form, but templates exist to conform to standards in different disciplines. Psychology registrations are presently the most numerous on the OSF ().</p>
<h2 id="meta-analysis-research">Meta-Analysis Research</h2>
<p>Another method of detecting and dealing with publication bias is to conduct meta-analysis. This method of research collects all published findings on a given topic, and analyzes the results collectively, and can detect, and attempt to adjust for, publication bias in the literature. Although quite common in medical research, the tool is not widely used in some parts of the social sciences. But even in economics, where many students are completely unfamiliar with the technqiue, important papers exist that have quantitatively synthesized bodies of literature. The unemployment effects of the minimum wage were meta-analyzed in <span class="citation"></span>, and the returns to education in <span class="citation"></span>. A meta-analysis of 87 meta-analyses in economics shows that publication bias is widespread, but not universal.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<p>ADD Medical, Psych, Poly Sci references</p>
<h1 id="rdof">Researcher Degrees of Freedom</h1>
<p>In addition to publication bias, a problem with research is specification searching: the manipulation of statistical or regression models unknowingly (or deliberately) until significance is obtained. Though registration helps solve the problem of publication bias, it does not solve the problem of fishing for statistical significance within a given study. <span class="citation"></span> refer to this as “researcher degrees of freedom,” and it has also been referred to as “fishing” or “p-hacking.” <span class="citation"></span> Using flexibility around when to stop collecting data, excluding certain observations, combining and comparing certain conditions, including certain control variables, and combining or transforming certain measures, they “prove” that listening to the Beatles’ song “When I’m Sixty-Four” made listeners a year and a half younger. The extent and ease of this “fishing” is also described in <span class="citation"></span>. <span class="citation"></span> agree that “<span>[</span>a<span>]</span> dataset can be analyzed in so many different ways (with the choices being not just what statistical test to perform but also decisions on what data to exclude or exclude, <span>[</span>sic<span>]</span> what measures to study, what interactions to consider, etc.), that very little information is provided by the statement that a study came up with a <em>p</em>&lt;.05 result.” However, they also conclude that:</p>
<blockquote>
<p>the term ``fishing&quot; was unfortunate, in that it invokes an image of a researcher trying out comparison after comparison, throwing the line into the lake repeatedly until a fish is snagged. We have no reason to think that researchers regularly do that. We think the real story is that researchers can perform a reasonable analysis given their assumptions and their data, but had the data turned out differently, they could have done other analyses that were just as reasonable in those circumstances.</p>
<p>We regret the spread of the terms ``fishing“ and “p-hacking” (and even “researcher degrees of freedom”) for two reasons: first, because when such terms are used to describe a study, there is the misleading implication that researchers were consciously trying out many different analyses on a single data set; and, second, because it can lead researchers who know they did not tryout many different analyses to mistakenly think they are not so strongly subject to problems of researcher degrees of freedom.”</p>
</blockquote>
<p>In other words, the problem is even worse than you think. What can be done to solve it? We believe part of the answer lies in detailed pre-analysis plans, described below.</p>
<h2 id="pre-analysis-plans">Pre-Analysis Plans</h2>
<p>While registration of studies can help to reduce publication bias or the file drawer problem, A pre-analysis plan (PAP), a detailed outline of the analyses that will be conducted in a study, can be used to reduce researcher degrees of freedom. While registration is now the norm in medicine for randomized trials, these often do not include detailed analysis plans. Sometimes this is because the medical researcher intends to do very little, if any, structural or formal modeling. But even if a researcher intends only to compare unadjusted means and bootstrap for standard errors, this should be explicitly stated. In the social sciences, this simple comparison is often not the end goal of a randomized trial, so there may be even more value in pre-specification of intended analyses. In the section below we explain what exactly should go into a pre-analysis plan.</p>
<p>A pre-analysis plan (PAP) contains a specification of the outcomes of the study, as well as a specification of the methods that will be used to analyze the outcomes (sometimes referred to as endpoints in the medical literature). By describing the method(s) of analysis ahead of time, and to some degree tying the hands of the researcher, we reduce the ability to data mine. Though one example of this exists in economics from 2001 <span class="citation"></span>, the idea is still quite new to the social sciences. The level of detail varies widely, and the research community is still constructing norms for incorporating these documents into final analyses and papers.</p>
<p>Suggestions have been made for the detailed contents of these documents. <span class="citation"></span> suggest including the following:</p>
<ol>
<li><p>the main outcome measures,</p></li>
<li><p>which outcome measures are primary and which are secondary,</p></li>
<li><p>the precise composition of any families that will be used for mean effects analysis,</p></li>
<li><p>the subgroups that will be analyzed,</p></li>
<li><p>the direction of expected impact if we want to use a one-sided test, and</p></li>
<li><p>the primary specification to be used for the analysis.</p></li>
</ol>
<p>David McKenzie of the World Bank Research Group proposed a list of ten items that should be included in a PAP, reproduced below. (For more detail see <a href="http://blogs.worldbank.org/impactevaluations/a-pre-analysis-plan-checklist" class="uri">http://blogs.worldbank.org/impactevaluations/a-pre-analysis-plan-checklist</a>)</p>
<ol>
<li><p>Description of the sample to be used in the study</p></li>
<li><p>Key data sources</p></li>
<li><p>Hypotheses to be tested throughout the causal chain</p></li>
<li><p>Specify how variables will be constructed</p></li>
<li><p>Specify the treatment effect equation to be estimated</p></li>
<li><p>What is the plan for how to deal with multiple outcomes and multiple hypothesis testing?</p></li>
<li><p>Procedures to be used for addressing survey attrition</p></li>
<li><p>How will the study deal with outcomes with limited variation?</p></li>
<li><p>If you are going to be testing a model, include the model</p></li>
<li><p>Remember to archive it</p></li>
</ol>
<p>In their article on researcher degrees of freedom, Simmons, Nelson, and Simonsohn (2011) suggest the following requirements for authors:</p>
<ol>
<li><p>Authors must decide the rule for terminating data collection before data collection begins and report this rule in the article.</p></li>
<li><p>Authors must collect at least 20 observations per cell or else provide a compelling cost-of-data-collection justification.</p></li>
<li><p>Authors must list all variables collected in a study.</p></li>
<li><p>Authors must report all experimental conditions, including failed manipulations.</p></li>
<li><p>If observations are eliminated, authors must also report what the statistical results are if those observations are included.</p></li>
<li><p>If an analysis includes a covariate, authors must report the statistical results of the analysis without the covariate.</p></li>
</ol>
<p><span class="citation"></span> also mention the “tension between the benefits of the credibility that comes from tying ones hands versus the benefits of flexibility to respond to unforeseen events and results.” Writing a PAP can lend extra credibility to research by making it of a confirmatory nature as opposed to an exploratory nature. Both types of research are absolutely valuable, but knowing the distinction is important. If some sort of restriction on the data, be it a specific functional form, exclusion of outliers, or an interaction term (subgroup analysis) that turns a null effect for the population into a significant effect for some subgroup, is specified ahead of time based on theory or previous research, this can be considered confirmatory research. Some would say this is of more value than the exploratory research approach of simply running 20 sub-group analyses and finding that one or two are significant. This may be an estimate of a true effect, but should be labeled as exploratory, and future researchers could attempt to confirm this finding by addressing the question of the sub-group specifically. The potential downside to pre-stating hypotheses and analysis plans is that no matter how carefully researchers plan ahead, something truly unexpected can occur. (An example discussed at a recent conference was subjects showing up for an experiment stoned. One example from a field experiment involved fatalities from a lightning strike at a school <span class="citation"></span>. ) This is why, even though we may use the phrase “bind our hands,” we advocate that researchers not be punished for conducting research outside the analysis plan. We simply recommend that researchers clearly delineate which analysis was included in the analysis plan, and which was not, so that readers can know what is confirmatory and what is exploratory.</p>
<p>There is some question as to when one should write one’s pre-analysis plan. “Before you begin to analyze your data” seems like the obvious answer, but this should be precisely defined. One could write the PAP before any baseline survey takes place, after any intervention but before endline, or after endline but before analysis has begun. <span class="citation"></span> has an informative discussion of the relative values of PAP timing. If one writes the PAP before the baseline, this is in some sense the purest, most free from accusations of p-hacking, but one could also miss valuable information. For example, suppose in baseline one learns that the intended outcome question is phrased poorly and elicits high rates of non-response, or that there is very little variation in the answers to a survey question. If the PAP was written after baseline, one could have accounted for this, but at the same time, researchers would also be free to change the scope of their analysis–for example, in the baseline survey of a field experiment designed to increase wages revealed that few of the subjects worked outside the home, the researcher could change the focus of the analysis. This is not necessarily wrong, but it does change the nature of the analysis somewhat.</p>
<p>PAPs could also be written after endline data has been collected but before the investigators have begun to analyze the data. Some have suggested that one could even look at baseline data from the control group only before writing the PAP. We find this problematic, however, since a researcher could learn that the control group had a particularly low or high value of a certain outcome variable, and then choose to include or not include this variable in the analysis as a result. The original research design could have been intended to analyze the increase in secondary school attendance, but looking at the control group, the researcher sees that the control group had a very high rate of attendance, making a significant difference between control and treatment (the treatment effect) unlikely. Learning this after the experiment has concluded and searching for things that might be easily different between treatment and control is more exploratory than confirmatory. An alternative proposal from Ben Olken at MIT is to remove the treatment status variable from the dataset before looking at the data, which seems to alleviate some of the concerns. However, one could still search for sub-group analyses at this stage. If you parse the outcome data by gender, and males and females have a similar distribution, to find a differential treatment effect by gender would seem unlikely. If male and female had wildly different outcomes, it would seem like a significant interaction is more likely. This is more exploratory than confirmatory.</p>
<h3 id="examples">Examples</h3>
<p>Examples of pre-analysis plans in the social sciences are relatively rare, but several examples of good published papers resulting from studies with PAP exist. Several of the items below come from the <a href="http://www.povertyactionlab.org/Hypothesis-Registry">J-PAL Hypothesis Registry</a>; we highlight those that have publicly available final papers.</p>
<ul>
<li><p><span class="citation"></span> includes evidence from a large-scale field experiment on community driven development projects in Sierra Leone. The analysis finds no significant benefits. Given the somewhat vague nature of the development projects that resulted from the funding, and the wide variety of potential outcomes, finding signficant results would have been relatively easy. In fact, the paper includes an example of how, if they had the latitude to define outcomes without a pre-analysis plan, the authors could have reported large and significantly positive outcomes. The paper also includes a discussion of the history and purpose of pre-analysis plans. The online appendix, available at <a href="http://emiguel.econ.berkeley.edu/assets/miguel_research/8/_Appendix__Reshaping_Institutions_-_Evidence__on__Aid__Impacts__Using__a__Pre___Analysis__Plan.pdf" class="uri">http://emiguel.econ.berkeley.edu/assets/miguel_research/8/_Appendix__Reshaping_Institutions_-_Evidence__on__Aid__Impacts__Using__a__Pre___Analysis__Plan.pdf</a>, contains the PAP.</p></li>
<li><p>Oregon expanded its medicare enrollment through a random lottery in 2008, providing researchers with an ideal avenue to evaluate the benefits of enrollment. <span class="citation"></span> show that recipients did not improve in physical health measurements, but were more likely to have insurance, had beter self-reported health outcomes, utilized emergency rooms more, and had better detection and management of diabetes. Pre-analsyis plans from the project are available at the National Bureau of Economics’ site devoted to the project: <a href="http://www.nber.org/oregon/documents.html" class="uri">http://www.nber.org/oregon/documents.html</a> (See, for example, <span class="citation"></span>.)</p></li>
<li><p>The shoe company Toms funded a rigorous evaluation of its in-kind shoe donation program. Researchers wrote a pre-analysis plan before conducting their research, and found no evidence that shoe donations displace local purchasing of shoes. See <span class="citation"></span>. The PAP is available at <a href="http://www.povertyactionlab.org/doc/pre-analysis-planwydick2-12-13pdf" class="uri">http://www.povertyactionlab.org/doc/pre-analysis-planwydick2-12-13pdf</a>. This is one of many projects that has benefited from a pre-analysis plan because of the involvement of a group with a vested interest, such as a government or corporation.</p></li>
<li><p>Researchers from UC San Diego and the World Bank evaluated job training programs run by the Turkish government and found only insignificant improvements and a strongly negative return on investment. See <span class="citation"></span>. The PAP is available in the J-PAL registry as well as at <a href="http://blogs.worldbank.org/impactevaluations/files/impactevaluations/iskurie_analysisplan_v4a.pdf" class="uri">http://blogs.worldbank.org/impactevaluations/files/impactevaluations/iskurie_analysisplan_v4a.pdf</a>).</p></li>
<li><p>Teams led by Ben Olken have evaluated multiple randomized interventions in Indonesia. The <a href="http://www.povertyactionlab.org/evaluation/project-generasi-conditional-community-block-grants-indonesia">Generasi program</a> linked community block grants to performance. The PAP are <span class="citation"></span> and are available in the J-PAL Hypothesis Registry <a href="http://www.povertyactionlab.org/Hypothesis-Registry" class="uri">http://www.povertyactionlab.org/Hypothesis-Registry</a>. The researchers found health improvement, but no education improvement <span class="citation"></span>.</p></li>
<li><p>Another project in Indonesia used a field experiment to evaluate different means of poverty targeting for cash transfer programs: proxy-means testing, community-based targeting, and a hybrid of the two. Results show that the proxy-means testing outperformed the other methods by 10%, but that community members were far more satisfied with the community method. The PAP and final paper are available as <span class="citation"></span> and <span class="citation"></span></p></li>
<li><p>An example from pyschology is a pre-registered replication of an implicit association test. Existing research showed evidence of stronger racial preferences among fertile women. <span class="citation"></span> failed to reproduce this effect in four tries, suggesting the association is weaker than originally found. The resulting manuscript, as well as the time-stamped registration of the analysis plan, can be found at <a href="https://osf.io/g3sca/" class="uri">https://osf.io/g3sca/</a></p></li>
</ul>
<p>A template</p>
<h2 id="multiple-hypothesis-testing">Multiple Hypothesis Testing</h2>
<p>Several of the PAP above, and several of the lists of suggestions above include corrections for multiple hypothesis testing. The idea of correcting for multiple tests is widespread in certain fields, but has yet to take hold in the social sciences. Simply put, the idea is that because we are aware of the fact that test statistics and p-values appear significant purely by chance a certain proportion of the time, we can report different, <em>better</em> p-values that control for the fact that we are running multiple tests. There are several ways to do this, all of which are used and explained in a simple and straightforward manner by <span class="citation"></span>:</p>
<ul>
<li><p>Report index tests—instead of reporting the outcomes of numerous tests, standardize outcomes and combine them into a smaller number of indeces (e.g. instead of separately reporting whether a long-term health intervention reduced blood pressure, diabetes, obesity, cancer, heart disease, and Alzheimer’s, report the results of a single health index.)</p></li>
<li><p>Control the Family-Wise Error Rate (FWER)—FWER is the probability that at least one true hypothesis in a group is rejected (a type I error), meaning it is advisable when the damage from falsely claiming <em>any</em> hypotheses are false is important. There are several ways to do this, with the simplest (but too conservative) method being the Bonferroni correction of simply multiplying every original p-value by the number of tests done. Holm’s sequential method involves ordering p-values by class and multiplying the lower p-values by higher dicsount factors <span class="citation"></span>. An efficient recent method is the free step-down resampling method, developed by <span class="citation"></span>.</p></li>
<li><p>Control the False Discovery Rate (FDR)—In situations where a single type I error is not catastrophic, researchers may be willing to use a less conservative method and trade off some incorrect rejections in exchange for greater power. This is possible by controlling the FDR, or the percentage of rejections that are type I errors. <span class="citation"></span> details a simple algorithm to control this rate at a chosen level, and <span class="citation"></span> described a two-step procedure with greater power.</p></li>
</ul>
<h2 id="subgroup-analysis">Subgroup Analysis</h2>
<p>One aspect of researcher degrees of freedom related to multiple hypothesis testing that seems to have taken hold widely in the medical literature is the aversion to sub-group analysis (“interactions” to most economists). Given the ability to test for a differential effect by many different groupings, crossed with each outcome variable, sub-groups analysis can almost always find some sort of supposedly significant effect. An oft-repeated story in the medical literature revolves around the publication of a study on aspirin after heart attacks. When the editors suggested including 40 subgroup analyses, the authors relented on the condition they include some of their own. Gemini and Libras had worse outcomes when taking aspirin after heart attacks, despite the large beneficial effects for everyone else. (Described in <span class="citation"></span>, with the original finding in <span class="citation"></span>) Whether in a randomized trial or not, we feel that social scientists could benefit from reporting the number of interactions tested, possibly adjusting for multiple hypotheses, and ideally specifying beforehand the interactions to be tested, with a justification from theory or previous evidence as to why the test is of interest.</p>
<h2 id="results-blind-reviewing">Results-Blind Reviewing</h2>
<p>A new development in research transparency which helps to address both publication bias and researcher degrees of freedom is results-blind reviewing. In results-blind reviewing, authors submit a detailed research plan <em>before</em> conducting the research. They submit this plan to a journal, and the journal rejects or gives an in-principle acceptance of the not-yet-written article based on the scientific merit of the question being asked and the methods proposed to answer them, as opposed to whether the results pass an arbitrary threshold of statistical significance. Then the authors conduct the research, and their paper is published as long as they don’t deviate too much from what they initially proposed. The editors and reviewers have tied their hands and have no ability to accept only significant results, and the authors have less incentive to game their statistical analysis in order to find something signficant.</p>
<p>This new mode of publication, called ``Registered Reports,&quot; is championed by Chris Chambers, psychologist at Cardiff University, and has been adopted by over a dozen journals. See a full list of journals adopting the procedure at <a href="https://osf.io/8mpji/" class="uri">https://osf.io/8mpji/</a>. <em>Social Pyschology</em> ran an issue dedicated to this type of article, with an editorial explaining the concept <span class="citation"></span>. <em>AIMS Neuroscience</em>, which uses this format, published an editorial answering 25 frequently asked questions pertaining to registered reports <span class="citation"></span>.</p>
<h1 id="replication-and-reproducibility">Replication and Reproducibility</h1>
<blockquote>
<p>“Economists treat replication the way teenagers treat chastity - as an ideal to be professed but not to be practised.”—Daniel Hamermesh, University of Texas at Austin Economics</p>
</blockquote>
<blockquote>
<p>“Reproducibility is just collaboration with people you don’t know, including yourself next week”—Philip Stark, University of California Berkeley Statistics</p>
</blockquote>
<p>Replication, in both practice and principle, is a key part of social science research. We first define what exactly we mean by replication using the taxonomy developed in <span class="citation"></span> and <span class="citation"></span>. Replication comes in a few different shapes: pure, statistical, and scientific.</p>
<ul>
<li><p>Pure: Using the exact same data and the exact same model to see if the published results are reproduced exactly.</p></li>
<li><p>Scientific: Using a different sample from a different population, and similar, but perhaps not identical model .</p></li>
<li><p>Statistical: Using the same model and underlying population but a different sample. In Hamermesh’s view, this is less relevant to certain fields, such as economics, where researchers are likely to already use as large a sample as is available.</p></li>
</ul>
<p>Others have described this in terms of a spectrum from full replication (independent collection of data and re-running analysis) to reproducibility, where the same data and code are re-used by other researchers <span class="citation"></span>. Whatever the terminology used, replication or reproducibility, transparent research requires making data and code available to other researchers so they can try and get the same results.</p>
<h2 id="project-protocols">Project Protocols</h2>
<p>A project protocol can be somewhat similar to a PAP, but is distinct. A protocol is a detailed recipe or instruction manual for others to use to reproduce an experiment. Protocols are important both in helping solve researcher degrees of freedom problems by making the exact details of analysis known and help avoid selective reporting, as well as in making one’s work reproducible. Protocols are standard in the medical literature, as in all areas of lab science, but may be less familiar to those used to working with administrative or observational data. Lab sciences are rife with examples of experiments failing to replicate because of supposedly minor changes such as the brand of bedding in mouse cages or the gender of the laboratory assistant or the speed at which one stirs a reagent <span class="citation"></span>, and we assume the same situation exists in the social sciences. <em>Nature</em> has decided to expand its methods section in order to encourage better reporting.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<p>We believe the social sciences would benefit from more careful documentation of methods. When one uses administrative data this can be accomplished by sharing one’s data and code so that analysis is transparent.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> This is discussed below in section [replication-and-reproducibility]. With original data collection, researchers should provide very detailed descriptions of what exactly they did. A 33-item checklist of suggested items is contained in the <a href="http://www.spirit-statement.org/">SPIRIT</a> (Standard Protocol Items: Recommendations for Interventional Trials) statement (see <a href="http://www.spirit-statement.org" class="uri">http://www.spirit-statement.org</a>, <span class="citation"></span>), including details on the participants, interventions, outcomes, assignment, blinding, data collection, data management, and statistical methods, among other things.</p>
<p>One area in which social sciences could improve involves details of randomization. <span class="citation"></span> documents the lack of clear explanation pertaining to how randomization was conducted in RCTs published in economics journals. Variables used for stratification are not described, and the decision of whether to control for baseline characteristics was often done after the fact. While there seems to be internal disagreement in both medicine and the social sciences over the appropriateness of including baseline control variables in regression analysis of a randomized trial, having researchers selectively report whatever method gives them the most significant-seeming results is obviously not the optimal outcome.</p>
<p>The medical literature also exhibits much greater concern over the blinding and concealment of randomized assignment than some of the social science literature. In some situations a social scientist interested in the potential scaling up of a government program may rightfully be unperturbed by a respondent assigned to the control group somehow gaining access to treatment, since this behavior would undoubtedly occur if the program were scaled and the researcher still has a valid intention to treat estimate, but this is clearly not always the case, especially if one wants an accurate estimate of the efficacy of a program or a treatment on the treated estimate. Tales of trials ruined through carelessness with the original randomization assignment as well as tips on how to avoid the same problem are described in <span class="citation"></span>. In addition to <span class="citation"></span>, political science has produced guidelines for randomization and related disclosure, avaialable at <a href="http://e-gap.org/resources/guides/randomization/" class="uri">http://e-gap.org/resources/guides/randomization/</a>.</p>
<p>Some medicine and science journals have begun to publish protocols. While the advantages of publishing a protocol related to the development of a new procedure (e.g. “we have developed a new method of isolating mRNA”) should be obvious, the advantages of publishing protocols for randomized trials under way are perhaps less obvious, but still exist. <em>BioMed Central</em> and <em>BMJ Open</em>, among others, now publish protocols of trials planned or ongoing, with the hopes that this will reduce publication bias, allow patients to see trials in which they might like to enroll, allow funders and researchers to learn of work underway to avoid duplication, and to allow readers to compare what research was originally proposed to what was actually completed. (See <a href="http://www.biomedcentral.com/authors/protocols" class="uri">http://www.biomedcentral.com/authors/protocols</a> and <a href="http://bmjopen.bmj.com/site/about/guidelines.xhtml#studyprotocols" class="uri">http://bmjopen.bmj.com/site/about/guidelines.xhtml#studyprotocols</a>.) <em>BMJ Open</em> suggests, but does not require, that its published protocols include the items in the SPIRIT checklist.</p>
<p>Even in published (or otherwise public) protocols, studies have found important differences between protocols and published results. 60-71% of outcomes described in protocols went unreported in the paper while 62% had major discrepancies between primary outcomes in the protocols and in the published papers, though there was a relatively even mix of these discrepancies favoring significant or insignificant results <span class="citation"></span>. Another study found that appropriate level of statistical detail is often lacking in protocols, and there are often discrepancies between protocols and published results <span class="citation"></span>. 31% of published papers had some sort of pre-specified plan for their regression adjustments (i.e. specifying which baseline covariates would be controlled for), while 70-74% of those that published a design paper or provided a protocol to the authors, but only 53% of the plans matched what was published in the ultimate paper.</p>
<h2 id="code-and-workflow">Code and Workflow</h2>
<p>Reproducing research often involves using the exact code and statistical programming done by the original researcher. To make this possible, code needs to be both (1) easily available and (2) easily interpretable. Thanks to several free and easy to use websites described below, code can easily be made available by researchers without requiring funding or website hosting. Making code easily interpretable is a more complicated task, nevertheless, the extra effort spent to make a more manageable code pays off with large dividends.</p>
<h3 id="publicly-sharing-code">Publicly Sharing Code</h3>
<p>Once analysis is complete (or even before this stage) researchers should share their data and code with the public. GitHub (<a href="http://www.github.com" class="uri">http://www.github.com</a>), The Center for Open Science’s Open Science Framework (<a href="http://osf.io" class="uri">http://osf.io</a>), and Harvard University’s Dataverse (<a href="http://thedata.org" class="uri">http://thedata.org</a>) are all free repositories for data and code that include easy to use version control.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> Version control is simply archiving previous versions of files so that old versions are not lost and can be returned to if needed. Instead of simply calling one’s analysis code “MyAnalysis.do” and repeatedly saving over and losing old versions, and instead of repeatedly changing the file name from “MyAnalysis.2014.8.13.do” to “MyAnalysis.2014.8.14.do” according to the date, version control creates different versions of files and can compare and highlight the differences in version of text files, and restore the used file to previous conditions if desired. Web services such as GitHub have the advantage of being “distributed” (DVCS) in that several users can have access simultaneously.</p>
<h3 id="managing-workflow">Managing Workflow</h3>
<p>Code is just one aspect of a larger structure we refer to as “workflow” after <span class="citation"></span> and <span class="citation"></span>, by which we mean the combination of data, code, organization, and documentation: everything from file and variable names to folder organization as well as efficient and readable programming, and data storage and documentation. We strongly recommend that Stata-users read <span class="citation"></span> and R users read <span class="citation"></span> for workflow recommendations both general and specific to their respective programming language. Our suggestions here borrw heavily from their excellent work.</p>
<h4 id="software">Software:</h4>
<p>Although we agree with the movement by many towards open source software such as <a href="http://www.r-project.org/">R</a> and <a href="https://www.python.org/">Python</a>, we appreciate that many disciplines have long traditions of using proprietary software such as SAS and STATA, and learning a new programming language may be an undesirable additional task in researchers’ busy lives. That said, there are several general coding rules that all researchers should use when organizing and implementing their analysis, and researchers should strive to make their work usable by as many others as possible.</p>
<h4 id="writing-code">Writing Code:</h4>
<p>Perhaps the most important rule is to write code, don’t work by hand. By that we mean:</p>
<ul>
<li><p>Do not modify data by hand, such as with a spreadsheet. Which is to say, don’t use Excel.</p></li>
<li><p>Use neither the command line nor drop-down menus nor point-and-click options in statistical software.</p></li>
<li><p>Instead, do everything with scripts.</p></li>
</ul>
<p>The simple reason for this is reproducibility. Modifying data in Excel or any similar spreadsheet program leaves no record of the changes made to the data, nor any explanation of the reasoning or timing behind any changes. Although it may seem easy or quick to do a one-time-only cleaning of data in Excel, or make “minor” changes to get the data into a format readable by a researcher’s preferred statistical software, unless these changes are written down in excruciating detail, this is not reproducible by other researchers. It is better to write a code script that imports the raw data, does all necessary changes, with comments in the code that explain changes, and saves any intermediate data sets used in analysis. Then, researchers can share their initial raw data and their code, and other researchers can reproduce their work exactly.</p>
<p>Though we understand that a fair amount of research has been done using pull down menus in SPSS or Stata, we advise against this. A bare minimum if one insists on going this route is to use the built-in command-logging features of the software. In Stata, this involves the ‘cmdlog’ command, in SPSS, this involves the paste button to add to a syntax.</p>
<p>The ideal is to make everything, including changes like rounding and formatting, done with scripts. Even downloading of data from websites can be done through a script. For example, in R, the download.file() function can be used to save data from a website. (Though of course this opens the possibility to the data file changing. When reproducing results from a given dataset is more important than the data from a specific source, researchers should download their raw dataset once, and never save over it, instead saving all modified intermediate datasets in a separate location.) Another extremely important way to prevent unintentional changes to data is to always set the seed for random number generators whenever any random numbers are to be used (set.seed() in R, set seed () in Stata). Additionally, information about the exact software version used should be included (Stata version 12.x, or use the session.info() command in R) as well as computer processor and operating system information. The casual programmer may assume that sophisticated software would always produce the exact same answer across multiple versions of software and platforms, but this is not the case.</p>
<p>Organize Your Work:</p>
<p>Don’t save output. Just save code and data that generates it.</p>
<p>Think about the entire pipeline. Terry White “Hit by a bus test”</p>
<h2 id="general-workflow-suggestions">General Workflow Suggestions:</h2>
<ul>
<li><p>Do not use spaces in directory or file names, as it complicates referring to them in certain software.</p></li>
<li><p>Use “naming directories”, .i.e. a directory beginning with “-” (so that it will appear first alphabetically) inside each directory to explain the contents of the above directory.</p></li>
<li><p>Add name, date, and describe contents, as well as updates, to all scripting files.</p></li>
<li><p>Keep a daily research log, i.e. a detailed written diary of what research is done on a given day. You’ll be surprised how often this will be useful to answer questions about whether you ran a certain test or not, when you did it, and what you called the file.</p></li>
<li><p>Make sure that all .do files are self-contained, do not require data in memory, or ideally, certain directory.</p></li>
<li><p>You can never comment too much.</p></li>
<li><p>Indent your code</p></li>
<li><p>Once you post/distribute code or data, any changes at all require a new file name.</p></li>
<li><p>Separate your cleaning and analysis files; don’t make any new variables that need saving (or will be used by a different analysis file) in an analysis file— it is better to only create them once so you know they’re the same.</p></li>
<li><p>Never name a file “final” because it won’t be.</p></li>
<li><p>Name variables “male” instead of “gender.”</p></li>
<li><p>Use a prefix such as x_ or temp_ so you know which files can easily be deleted.</p></li>
<li><p>Never change the contents of a variable unless you give it a new name.</p></li>
<li><p>Every variable should have a label.</p></li>
</ul>
<h2 id="stata-specific-suggestions">Stata-specific Suggestions</h2>
<ul>
<li><p>Use the different missing values (“.a”-“.z”, not exclusively “.”) in order to distinguish between “don’t know” and “didn’t ask” or other distinct reasons for missing data.</p></li>
<li><p>Make sure code always produces same result—set seed and sort/merge stable</p></li>
<li><p>Use the ‘version’ command in your .do file to ensure that other researchers who run your code with a newer version of Stata get the same results.</p></li>
<li><p>Don’t use abbreviations for variables ( which may become unstable after adding variables) or commands (beyond reason)</p></li>
<li><p>Avoid using global macros. (This is a common piece of programming advice.)</p></li>
<li><p>Use locals for varlists to ensure that long lists of variables include the same variables whenever intended.</p></li>
<li><p>Use ‘return’ command instead of typing in numbers</p></li>
<li><p>If you have a master .do file that calls other .do files, which each have their own .log file, you can run multiple log files at the same time (so you have a master .log file)</p></li>
<li><p>Use the ‘label data’ and ‘notes’ commands to label datasets and help yourself and other researchers easily identify the contents.</p></li>
<li><p>Use the ‘notes’ command for variables as well for identifying information that is too long for the variable label.</p></li>
<li><p>Use the ‘datasignature’ command to generate a hash and help ensure that data is the same as before.</p></li>
<li><p>Use value labels for all categorical variables, but include the numerical value in the label.</p></li>
<li><p>Even though Stata is case sensitive, don’t use capital letters in variable names since not all software packages are case sensitive.</p></li>
<li><p>Make your files as non-proprietary as possible (use the ‘saveold’ command to enable those with earlier versions to use your data. This is why trusted repositories are so useful–they’ll do this for you.)</p></li>
</ul>
<p>In addition to making code available to the public, the code itself should be written in a reader-friendly format, referred to as “Literate Programming,” introduced in <span class="citation"></span> and <span class="citation"></span>. The basic idea is that “the time is ripe for significantly better documentation of programs, and that we can best achieve this by considering programs to be <em>works of literature</em>…Instead of imagining that our main task is to instruct a <em>computer</em> what to do, let us concentrate rather on explaining to <em>human beings</em> what we want a computer to do.” <span>[</span>emphasis original<span>]</span> Simply put, code should be written in as simple and easily understood a way as possible, and should be very well commented, so that researchers other than the original author can more easily understand the goal of the code.</p>
<p>One tool to make literate (statistical) programming significantly easier is Knitr (see <span class="citation"></span>) which is built into R Studio<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a>. Knitr uses R Markdown (a very simple plain text markup language, described at <a href="http://rmarkdown.rstudio.com/" class="uri">http://rmarkdown.rstudio.com/</a>) in which one writes both code and comments that is automatically spun into an easily read and shareable HTML, PDF, or MS Word document. These can be posted and shared for free at RPubs (<a href="https://rpubs.com" class="uri">https://rpubs.com</a>), an easy to use hosting service by Rstudio.</p>
<h2 id="sharing-data">Sharing Data</h2>
<p>In addition to code, researchers should share their data if at all possible. Many journals do not require sharing of code, but the number that do is increasing.</p>
<h3 id="the-jmcb-project-and-economics">The JMCB Project and Economics</h3>
<p>In the field of economics, few, if any journals required sharing of data before “The Journal of Money, Credit, and Banking Project,” published in <em>The American Economic Review</em> in 1986 <span class="citation"></span>. <em>The Journal of Money, Credit, and Banking</em> started the <em>JMCB Data Storage and Evaluation Project</em> with NSF funding in 1982, which requested data and code from authors who published in the journal. With a great deal of research funded by the NSF, it should be noted that they have long had an explicit policy of expecting researchers to share their primary data<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a>. Despite this, and despite the explicit policy of the <em>Journal</em> during the project, at most only 78% of authors provided data to the authors within six months after multiple requests. (This is admittedly an improvement over the 34% from the control group—those who published before the <em>Journal</em> policy went into effect—who provided data.) Of the papers that were still under review by the <em>Journal</em> at the time of the requests for data, one quarter did not even respond to the request, despite the request coming from the same journal considering their paper! The submitted data was often an unlabeled and undocumented mess. Despite this, the authors attempted to replicate nine papers, and often were completely unable to reproduce published results, despite detailed assistance from the original authors.</p>
<p>Shockingly, nothing much changed with the publication of this important article. A decade later, in a follow-up piece to the JMCB Project published in the Federal Reserve Bank of St. Louis <em>Review</em> <span class="citation"></span>, the authors note that only two economics journals other than the <em>Review</em> itself (<em>Journal of Applied Econometrics, Journal of Business and Economic Statistics</em>) requested data from authors, and neither requested code. The <em>JMCB</em> itself discontinued the policy of requesting data in 1993, though it resumed requesting data in 1996. The authors repeated their experiment with papers presented at the St. Louis Federal Reserve Bank conference in 1992, and obtained similar response rates as original JMCB Project. The flagship economics journal, the <em>American Economic Review</em> (AER), did not start requesting data until 2003. Finally, after a 2003 article showing that nonlinear maximization methods often produce wildly different estimates across different software packages, that not a single AER article tested their solution with different software, and that fully half of queried authors from a chosen issue of the AER, including a then editor of the journal, failed to comply with the policy of providing data and code, editor Ben Bernanke made the data and code policy mandatory in 2005 <span class="citation"></span>.</p>
<p>The current data policy from the <em>American Economic Review</em> can be seen here: <a href="https://www.aeaweb.org/aer/data.php" class="uri">https://www.aeaweb.org/aer/data.php</a>. In addition to all the journals published by the American Economic Association, several top journals, including <em>Econometrica, The Journal of Applied Econometrics, The Journal of Money Credit and Banking, the Journal of Political Economy, The Review of Economics and Statistics, and the Review of Economic Studies</em>, now explicitly require data and code to be submitted at the time of publication. The AER conducted a review and found good, but incomplete, compliance <span class="citation"></span>.</p>
<h3 id="general-repositories">General Repositories</h3>
<p>The previous section on the <em>JMCB</em> describes only a few journals in one field of the social sciences. Even if the journal to which you submit your research does not require you to supply them with your code and data, researchers should still share these things. Though some repositories, particularly Harvard’s Dataverse, seem equipped to handle data from practically any researcher (a free 1 TB of storage is standard, with more possible upon request), many repositories specialize. The Registry of Research Data Repositories (<a href="http://www.re3data.org" class="uri">http://www.re3data.org</a>) has described over 900 data repositories to help you find the right data repository for your data. A key advantage to using a trusted repository such as one listed here, in lieu of simply throwing the data up on your own website or making your Dropbox folder public, is that many of these repositories will take your data in its proprietary (Stata, SAS, SPSS, etc.) form, and make it accessible in other formats. Storing your data in a repository with other similar datasets also makes it easier for others to find your data, instead of requiring that they already know of its existence, as would likely be the case with personal websites. Your own personal website is also more likely to be taken offline, should a researcher change schools or retire.</p>
<h3 id="differential-privacy">Differential Privacy</h3>
<p>One important caveat to making data widely available, is that despite anonymization, in the age of big data, sometimes individual subjects can easily be identified. <span class="citation"></span> recount deliberate data releases by Yahoo! Inc., the Massachusetts state government, and Netflix, that could easily be used to identify individuals in the data, despite the absence of direct identifiers such as names or social security numbers. The problem is that “de-identification does not guarantee anonymization.” This problem is well known in computer science, but solutions are not yet agreed upon, nor widely implemented.</p>
<h2 id="reporting-standards">Reporting Standards</h2>
<p>In research, the devil truly is in the details. Whether it is for assessing the validity of a research design or for attempting to replicate a study, details of what exactly was done must be recorded and made available to other researchers. The exact details that are relevant will likely differ from field to field, but an increasing number of fields have produced centralized checklists that describe (in excruciating detail) what disclosure is required of published studies. These checklists are not often published with the paper, but can be submitted with the original article so that reviewers can check that it has been completed. With infinite and easy web storage, researchers can easily post these materials on their website even if journal editors insist on cutting their methods sections for space reasons.</p>
<h3 id="randomized-trials">Randomized Trials and CONSORT</h3>
<p>The most widely adopted reporting standard guideline is the Consolidated Standards of Reporting Trials (CONSORT), available at <a href="http://www.consort-statement.org" class="uri">http://www.consort-statement.org</a>. Parallel to construction of <a href="http://clinicaltrials.gov">clinicaltrials.gov</a> and registration, reporting standards evolved, and are now nearly universally adopted for randomized trials published in medical journals, required or requested by reviewers during the review process. This is still in its infancy in the social sciences.</p>
<p>The original CONSORT was developed in the mid 1990’s <span class="citation"></span>. After five years, research showed that reporting of essential details, as required by the checklist, had significantly increased in journals requiring the standard <span class="citation"></span>. The statement was revised in 2001, and simultaneously published in three of the top journals <span class="citation"></span>). The statement was again revised in 2010 <span class="citation"></span>. The statement is a 25-item checklist pertaining to the title, abstract, introduction, methods, results, and discussion of the article in question, and seeks to delineate the minimum requirements of disclosure that may not be sufficiently addressed through other measures.</p>
<h2 id="soc-sci-standards">Social Science Reporting Standards</h2>
<p>Though a standard akin to CONSORT has not been formally adopted by social science or behavioral science journals, at least as far as we are aware, there have been attempts to do this: In political science, the Experimental Research Section Standards Committee produced a detailed list of items required for disclosure of experiments in political science <span class="citation"></span>. This checklist is available <a href="http://www.davidhendry.net/research-supplemental/gerberetal2014-reportingstandards/gerberetal2014-reportingstandards&amp;appendix1.pdf">here</a>.<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p>
<p>In economics, one article has highlighted the fact that there is not much discussion of essential features of randomization (how was randomization stratified, if at all? How were control variables determined?), but no standards have been adopted. <span class="citation"></span></p>
<p>In psychological and behavioral research, an extention to CONSORT for Social and Psychological Interventions (CONSORT-SPI) was developed in <span class="citation"></span>, but has so far not been widely adopted, or required by journals.</p>
<h2 id="observational-standards">Observational Reporting Standards</h2>
<p>Social science has yet to make a serious push for reporting standards in observational work, but the medical/epidemiological literature has created standards in this type of work, though they are not as widely adopted as CONSORT. Perhaps the most well-known is the STROBE Statement (Strengthening the reporting of observational studies in epidemiology), available at <a href="http://www.strobe-statement.org" class="uri">http://www.strobe-statement.org</a>. STROBE provides checklists for reporting of cohort, case-control, and cross-sectional studies. These standards have been endorsed by approximately 100 journals in the field.<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a></p>
<p>Medicine has in fact come up with too many checklists to describe them all individually. Acknowledging that every field and type of research is different, the Equator Network (Enhancing the Quality of Transparency of Health Research) serves as an umbrella organization that seeks to keep tabs on all the best reporting standards and help researchers find which reporting standard is most relevant for their research. See <a href="http://www.equator-network.org/" class="uri">http://www.equator-network.org/</a> for more information.</p>
<h1 id="conclusion">Conclusion</h1>
<p>As you may have noticed, many of the activities described in this manual require extra work. Before you run an experiment, we’re telling you to write down the hypothesis, carefully explain how you are going to test the hypothesis, write down the very regression analysis you’re going to run, write a detailed protocol of the exact experimental setting, and then you have to post all of this publicly on the Internet with some sort of Big Brother organization. Or at least that’s one way to look at it. But we strongly belive these steps are (1) not that difficult once you get used to them and (2) well worth the reward. You’ll get p-values you can believe in. The next time someone asks you for your data, you just point them to the website, where they’ll download the data and code, and the code will produce the exact results in the published paper. The next time you open up a coding file you haven’t looked at in months to make a change suggested by a reviewer, your code will be so thoroughly commented, you’ll know exactly where to go to make the changes. And the next time you want to extend the analysis of a published paper, you click the link in the paper and have the data on your own computer in seconds. Science moves forward.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>HHS and NIH took steps in November 2014 to expand the amount of results reporting required. See <a href="http://www.nih.gov/news/health/nov2014/od-19.htm" class="uri">http://www.nih.gov/news/health/nov2014/od-19.htm</a><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Other less-widely adopted attempts to create registries in political science are the Political Science Registered Studies Dataverse (PSRSD, <a href="../customXml/item1.xml">http://spia.uga.edu/faculty_pages/monogan/registration.php</a>) and the PAP Registry of the Experimental Research section of the American Political Science Association (<a href="numbering.xml">http://ps-experiments.ucr.edu/browser</a>).<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>A helpful resource, which includes meta-analysis datasets for economics researchers interested in conducting a meta-analysis is available <a href="http://www.deakin.edu.au/business/economics/research/meta-analysis">here</a>. Also see <span class="citation"></span>, which helpfully describes the tools of meta-analysis, and is part of a special issue of <em>The Journal of Economic Surveys</em> dedicated to meta-analysis.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p><a href="http://www.nature.com/news/announcement-reducing-our-irreproducibility-1.12852" class="uri">http://www.nature.com/news/announcement-reducing-our-irreproducibility-1.12852</a><a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>It should be noted that the need for documentation of survey method is not eliminated by using administrative data, the burden simply falls upon the administration.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>BitBucket (<a href="styles.xml">http://www.bitbucket.org</a>) XXX, and XXX are also web services that one can use for free version control and archiving of public data and code.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>R Studio is a popular free integrated implementation of R, available at <a href="stylesWithEffects.xml">http://www.rstudio.com</a>.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>“Investigators are expected to share with other researchers, at no more than incremental cost and within a reasonable time, the primary data, samples, physical collections and other supporting materials created or gathered in the course of work under NSF grants. Grantees are expected to encourage and facilitate such sharing.” See http://www.nsf.gov/bfa/dias/policy/dmp.jsp<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p><a href="http://www.davidhendry.net/research-supplemental/gerberetal2014-reportingstandards/gerberetal2014-reportingstandards&amp;appendix1.pdf" class="uri">http://www.davidhendry.net/research-supplemental/gerberetal2014-reportingstandards/gerberetal2014-reportingstandards&amp;appendix1.pdf</a><a href="#fnref9">↩</a></p></li>
<li id="fn10"><p><a href="http://www.strobe-statement.org/index.php?id=strobe-endorsement" class="uri">http://www.strobe-statement.org/index.php?id=strobe-endorsement</a><a href="#fnref10">↩</a></p></li>
</ol>
</div>
